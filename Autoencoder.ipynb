{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(\"NSL_KDD/20 Percent Training Set.csv\", \"r\")\n",
    "train_data = [x.rstrip().split(\",\") for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)-1, -1, -1):\n",
    "    if(train_data[i][41] != 'normal'):\n",
    "        train_data.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto = ['tcp', 'udp', 'icmp']\n",
    "service = ['finger', 'nnsp', 'private', 'ftp_data', 'exec', 'http', 'smtp', 'ftp', 'other', 'eco_i', 'vmnet', 'telnet', 'domain_u', 'ctf', 'ssh', 'urp_i', 'uucp_path', 'name', 'mtp', 'ldap', 'supdup', 'discard', 'http_443', 'efs', 'IRC', 'ecr_i', 'auth', 'bgp', 'hostnames', 'iso_tsap', 'domain', 'imap4', 'echo', 'nntp', 'sunrpc', 'systat', 'csnet_ns', 'netbios_ssn', 'gopher', 'X11', 'uucp', 'whois', 'klogin', 'time', 'login', 'netbios_dgm', 'netstat', 'daytime', 'netbios_ns', 'kshell', 'Z39_50', 'link', 'printer', 'pop_2', 'ntp_u', 'courier', 'rje', 'pop_3', 'sql_net', 'remote_job', 'urh_i', 'red_i', 'shell', 'pm_dump', 'tim_i', 'http_8001', 'tftp_u', 'aol', 'http_2784', 'harvest']\n",
    "flag = ['S0', 'REJ', 'SF', 'RSTO', 'S1', 'RSTR', 'S2', 'SH', 'OTH', 'RSTOS0', 'S3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = open(\"NSL_KDD/KDDTest+.csv\", \"r\")\n",
    "test_data = [x.rstrip().split(\",\") for x in test_data]\n",
    "\n",
    "train_data = [x[:41] for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_dict = {}\n",
    "service_dict = {}\n",
    "flag_dict = {}\n",
    "\n",
    "for i in proto:\n",
    "    proto_dict[i] = []\n",
    "    for x in range(len(proto)):\n",
    "        if(proto[x] == i):\n",
    "            proto_dict[i].append(1.0)\n",
    "        else:\n",
    "            proto_dict[i].append(0.0)\n",
    "            \n",
    "for i in service:\n",
    "    service_dict[i] = []\n",
    "    for x in range(len(service)):\n",
    "        if(service[x] == i):\n",
    "            service_dict[i].append(1.0)\n",
    "        else:\n",
    "            service_dict[i].append(0.0)\n",
    "            \n",
    "for i in flag:\n",
    "    flag_dict[i] = []\n",
    "    for x in range(len(flag)):\n",
    "        if(flag[x] == i):\n",
    "            flag_dict[i].append(1.0)\n",
    "        else:\n",
    "            flag_dict[i].append(0.0)\n",
    "            \n",
    "            \n",
    "# NEED TO ALSO PREPROCESS EACH FEATURE TO BE BETWEEN 0 AND 1???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for idx1, i in enumerate(train_data):\n",
    "    new.append([])\n",
    "    for idx2, x in enumerate(i):\n",
    "        if(idx2 == 1):\n",
    "            for z in proto_dict[x]:\n",
    "                new[idx1].append(z)\n",
    "        elif(idx2 == 2):\n",
    "            for z in service_dict[x]:\n",
    "                new[idx1].append(z)\n",
    "        elif(idx2 == 3):\n",
    "            for z in flag_dict[x]:\n",
    "                new[idx1].append(z)\n",
    "        else:\n",
    "            new[idx1].append(float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_inpl = len(new[0])\n",
    "n_nodes_hl1 = 50\n",
    "n_nodes_hl2 = 32\n",
    "n_nodes_hl3 = 32\n",
    "n_nodes_outl = len(new[0])\n",
    "\n",
    "hidden_1_layer_vals = {\n",
    "    'weights':tf.Variable(tf.random_normal([n_nodes_inpl,n_nodes_hl1])),\n",
    "    'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))\n",
    "}\n",
    "\n",
    "hidden_2_layer_vals = {\n",
    "    'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "    'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))\n",
    "}\n",
    "\n",
    "hidden_3_layer_vals = {\n",
    "    'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "    'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))\n",
    "}\n",
    "\n",
    "output_layer_vals = {\n",
    "    'weights':tf.Variable(tf.random_normal([n_nodes_hl3,n_nodes_outl])),\n",
    "    'biases':tf.Variable(tf.random_normal([n_nodes_outl]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.placeholder('float', [None, n_nodes_inpl])\n",
    "\n",
    "layer_1 = tf.nn.sigmoid(\n",
    "    tf.add(tf.matmul(input_layer,hidden_1_layer_vals['weights']),\n",
    "    hidden_1_layer_vals['biases']))\n",
    "\n",
    "layer_2 = tf.nn.sigmoid(\n",
    "    tf.add(tf.matmul(layer_1,hidden_2_layer_vals['weights']),\n",
    "    hidden_2_layer_vals['biases']))\n",
    "\n",
    "layer_3 = tf.nn.sigmoid(\n",
    "    tf.add(tf.matmul(layer_2,hidden_3_layer_vals['weights']),\n",
    "    hidden_3_layer_vals['biases']))\n",
    "\n",
    "output_layer = tf.matmul(layer_3,output_layer_vals['weights']) + output_layer_vals['biases']\n",
    "\n",
    "output_true = tf.placeholder('float', [None, n_nodes_outl])\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "# define our optimizer\n",
    "learn_rate = 1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 loss: 36102490357.16406\n",
      "Epoch 20 / 500 loss: 36078106420.875\n",
      "Epoch 40 / 500 loss: 36066949498.21094\n",
      "Epoch 60 / 500 loss: 36058676249.02344\n",
      "Epoch 80 / 500 loss: 36051696413.953125\n",
      "Epoch 100 / 500 loss: 36046184216.41406\n",
      "Epoch 120 / 500 loss: 36040487469.53125\n",
      "Epoch 140 / 500 loss: 36035361597.17969\n",
      "Epoch 160 / 500 loss: 36030784465.72266\n",
      "Epoch 180 / 500 loss: 36026462296.38672\n",
      "Epoch 200 / 500 loss: 36022457502.80078\n",
      "Epoch 220 / 500 loss: 36018606795.5\n",
      "Epoch 240 / 500 loss: 36015175197.55078\n",
      "Epoch 260 / 500 loss: 36011449904.38672\n",
      "Epoch 280 / 500 loss: 36008113398.08203\n",
      "Epoch 300 / 500 loss: 36004913058.09766\n",
      "Epoch 320 / 500 loss: 36001769909.99219\n",
      "Epoch 340 / 500 loss: 35998776461.58594\n",
      "Epoch 360 / 500 loss: 35995878762.65234\n",
      "Epoch 380 / 500 loss: 35993085483.0625\n",
      "Epoch 400 / 500 loss: 35990376788.60156\n",
      "Epoch 420 / 500 loss: 35987679647.07031\n",
      "Epoch 440 / 500 loss: 35985152181.57422\n",
      "Epoch 460 / 500 loss: 35982717897.09375\n",
      "Epoch 480 / 500 loss: 35979794386.921875\n",
      "Training time: 216.06799411773682 seconds\n"
     ]
    }
   ],
   "source": [
    "# initialising stuff and starting the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# defining batch size, number of epochs and learning rate\n",
    "batch_size = 100  # how many row to use together for training\n",
    "hm_epochs = 500    # how many times to go through the entire dataset\n",
    "tot_rows = len(train_data) # total number of data\n",
    "# running the model for a 100 epochs taking 100 row in batches\n",
    "# total improvement is printed out after each epoch\n",
    "start_time = time.time()\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "    for i in range(int(tot_rows/batch_size)):\n",
    "        epoch_x = new[ i*batch_size : (i+1)*batch_size ]\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "        epoch_loss += c\n",
    "    if(epoch % 20 == 0):\n",
    "        print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)\n",
    "print(\"Training time:\", time.time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = []\n",
    "for idx1, i in enumerate(test_data):\n",
    "    new_test.append([])\n",
    "    for idx2, x in enumerate(i):\n",
    "        if(idx2 >= 41):\n",
    "            break\n",
    "        if(idx2 == 1):\n",
    "            for z in proto_dict[x]:\n",
    "                new_test[idx1].append(z)\n",
    "        elif(idx2 == 2):\n",
    "            for z in service_dict[x]:\n",
    "                new_test[idx1].append(z)\n",
    "        elif(idx2 == 3):\n",
    "            for z in flag_dict[x]:\n",
    "                new_test[idx1].append(z)\n",
    "        else:\n",
    "            new_test[idx1].append(float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32391.275\n",
      "Testing time: 21.194310188293457 seconds\n",
      "TP:  1758\n",
      "TN:  7195\n",
      "FP:  2515\n",
      "FN:  11075\n",
      "Accuracy:  0.39715210930222244\n",
      "Recall:  0.13699057118366711\n",
      "Precision:  0.4114205476246197\n",
      "False Alarm Rate:  0.2590113285272915\n"
     ]
    }
   ],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "err_list = []\n",
    "\n",
    "start_test = time.time()\n",
    "\n",
    "for i in new:\n",
    "    curr_err = sess.run(meansq, feed_dict={input_layer: [i], output_true: [i]})\n",
    "    err_list.append(curr_err)\n",
    "    \n",
    "sorted_errs = sorted(err_list)\n",
    "Q3_err = sorted_errs[int(len(err_list)*3/4)]\n",
    "Q1_err = sorted_errs[int(len(err_list)/4)]\n",
    "err_threshold = Q3_err\n",
    "print(err_threshold)\n",
    "\n",
    "# plt.plot([idx*5 for idx in range(len(err_list))], err_list, 'b.')\n",
    "\n",
    "for idx, i in enumerate(test_data):\n",
    "    if(i[41] == 'normal'):\n",
    "        cr = sess.run(meansq, feed_dict={input_layer: [new_test[idx]], output_true: [new_test[idx]]})\n",
    "        if(cr > err_threshold):\n",
    "            # Outlier\n",
    "            FP += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    else:\n",
    "        cr = sess.run(meansq, feed_dict={input_layer: [new_test[idx]], output_true: [new_test[idx]]})\n",
    "        # plt.plot([5*idx+len(err_list)+1], [cr], 'r.')\n",
    "        if(cr > err_threshold):\n",
    "            # Outlier\n",
    "            TP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "\n",
    "print(\"Testing time:\", time.time() - start_test, \"seconds\")\n",
    "\n",
    "# Output the measurement\n",
    "print(\"TP: \", TP)\n",
    "print(\"TN: \", TN)\n",
    "print(\"FP: \", FP)\n",
    "print(\"FN: \", FN)\n",
    "print(\"Accuracy: \", (TP + TN) / (TP + TN + FP + FN))\n",
    "print(\"Recall: \", TP / (TP + FN))\n",
    "print(\"Precision: \", TP / (TP + FP))\n",
    "print(\"False Alarm Rate: \", FP / (FP + TN))\n",
    "\n",
    "# x_axis = np.arange(0.0, 140000.0, 1)\n",
    "# plt.plot(x_axis, [err_threshold for i in range(len(x_axis))], 'k_')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
